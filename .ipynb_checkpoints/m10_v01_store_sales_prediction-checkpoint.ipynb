{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0.0. IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-23T02:14:35.724026Z",
     "start_time": "2023-04-23T02:14:31.938950Z"
    }
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'GridSpec' from 'matplotlib' (C:\\Users\\micro\\anaconda3\\envs\\ds_em_producao\\lib\\site-packages\\matplotlib\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 15\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mboruta\u001b[39;00m                \u001b[38;5;28;01mimport\u001b[39;00m BorutaPy\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m            \u001b[38;5;28;01mimport\u001b[39;00m pyplot \u001b[38;5;28;01mas\u001b[39;00m plt\n\u001b[1;32m---> 15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m            \u001b[38;5;28;01mimport\u001b[39;00m GridSpec\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mIPython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdisplay\u001b[39;00m       \u001b[38;5;28;01mimport\u001b[39;00m Image\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mIPython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdisplay\u001b[39;00m  \u001b[38;5;28;01mimport\u001b[39;00m HTML\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'GridSpec' from 'matplotlib' (C:\\Users\\micro\\anaconda3\\envs\\ds_em_producao\\lib\\site-packages\\matplotlib\\__init__.py)"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import numpy  as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import pickle\n",
    "import requests\n",
    "import warnings\n",
    "import inflection\n",
    "import seaborn as sns\n",
    "import xgboost as xgb\n",
    "\n",
    "from scipy                 import stats  as ss\n",
    "from boruta                import BorutaPy\n",
    "from matplotlib            import pyplot as plt\n",
    "from matplotlib            import GridSpec\n",
    "from IPython.display       import Image\n",
    "from IPython.core.display  import HTML\n",
    "\n",
    "\n",
    "from sklearn.metrics       import mean_absolute_error, mean_squared_error\n",
    "from sklearn.ensemble      import RandomForestRegressor\n",
    "from sklearn.linear_model  import LinearRegression, Lasso\n",
    "from sklearn.preprocessing import RobustScaler, MinMaxScaler, LabelEncoder\n",
    "\n",
    "warnings.filterwarnings( 'ignore' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.1. Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-23T02:14:35.733021Z",
     "start_time": "2023-04-23T02:14:35.733021Z"
    }
   },
   "outputs": [],
   "source": [
    "def cross_validation( x_training, kfold, model_name, model, verbose=False ):\n",
    "    mae_list = []\n",
    "    mape_list = []\n",
    "    rmse_list = []\n",
    "    for k in reversed( range( 1, kfold+1 ) ):\n",
    "        if verbose:\n",
    "            print( '\\nKFold Number: {}'.format( k ) )\n",
    "        # start and end date for validation \n",
    "        validation_start_date = x_training['date'].max() - datetime.timedelta( days=k*6*7)\n",
    "        validation_end_date = x_training['date'].max() - datetime.timedelta( days=(k-1)*6*7)\n",
    "\n",
    "        # filtering dataset\n",
    "        training = x_training[x_training['date'] < validation_start_date]\n",
    "        validation = x_training[(x_training['date'] >= validation_start_date) & (x_training['date'] <= validation_end_date)]\n",
    "\n",
    "        # training and validation dataset\n",
    "        # training\n",
    "        xtraining = training.drop( ['date', 'sales'], axis=1 ) \n",
    "        ytraining = training['sales']\n",
    "\n",
    "        # validation\n",
    "        xvalidation = validation.drop( ['date', 'sales'], axis=1 )\n",
    "        yvalidation = validation['sales']\n",
    "\n",
    "        # model\n",
    "        m = model.fit( xtraining, ytraining )\n",
    "\n",
    "        # prediction\n",
    "        yhat = m.predict( xvalidation )\n",
    "\n",
    "        # performance\n",
    "        m_result = ml_error( model_name, np.expm1( yvalidation ), np.expm1( yhat ) )\n",
    "\n",
    "        # store performance of each kfold iteration\n",
    "        mae_list.append(  m_result['MAE'] )\n",
    "        mape_list.append( m_result['MAPE'] )\n",
    "        rmse_list.append( m_result['RMSE'] )\n",
    "\n",
    "    return pd.DataFrame( {'Model Name': model_name,\n",
    "                          'MAE CV': np.round( np.mean( mae_list ), 2 ).astype( str ) + ' +/- ' + np.round( np.std( mae_list ), 2 ).astype( str ),\n",
    "                          'MAPE CV': np.round( np.mean( mape_list ), 2 ).astype( str ) + ' +/- ' + np.round( np.std( mape_list ), 2 ).astype( str ),\n",
    "                          'RMSE CV': np.round( np.mean( rmse_list ), 2 ).astype( str ) + ' +/- ' + np.round( np.std( rmse_list ), 2 ).astype( str ) }, index=[0] )\n",
    "\n",
    "\n",
    "def mean_percentage_error( y, yhat ):\n",
    "    return np.mean( ( y - yhat ) / y )\n",
    "     \n",
    "    \n",
    "def mean_absolute_percentage_error( y, yhat ):\n",
    "    return np.mean( np.abs( ( y - yhat ) / y ) )\n",
    "\n",
    "    \n",
    "def ml_error( model_name, y, yhat ):\n",
    "    mae = mean_absolute_error( y, yhat )\n",
    "    mape = mean_absolute_percentage_error( y, yhat )\n",
    "    rmse = np.sqrt( mean_squared_error( y, yhat ) )\n",
    "    \n",
    "    return pd.DataFrame( { 'Model Name': model_name, \n",
    "                           'MAE': mae, \n",
    "                           'MAPE': mape,\n",
    "                           'RMSE': rmse }, index=[0] )\n",
    "\n",
    "def cramer_v( x, y ):\n",
    "    cm = pd.crosstab( x, y ).as_matrix()\n",
    "    n = cm.sum()\n",
    "    r, k = cm.shape\n",
    "    \n",
    "    chi2 = ss.chi2_contingency( cm )[0]\n",
    "    chi2corr = max( 0, chi2 - (k-1)*(r-1)/(n-1) )\n",
    "    \n",
    "    kcorr = k - (k-1)**2/(n-1)\n",
    "    rcorr = r - (r-1)**2/(n-1)\n",
    "    \n",
    "    return np.sqrt( (chi2corr/n) / ( min( kcorr-1, rcorr-1 ) ) )\n",
    "\n",
    "\n",
    "\n",
    "def jupyter_settings():\n",
    "    %matplotlib inline\n",
    "    %pylab inline\n",
    "    \n",
    "    plt.style.use( 'bmh' )\n",
    "    plt.rcParams['figure.figsize'] = [25, 12]\n",
    "    plt.rcParams['font.size'] = 24\n",
    "    \n",
    "    display( HTML( '<style>.container { width:100% !important; }</style>') )\n",
    "    pd.options.display.max_columns = None\n",
    "    pd.options.display.max_rows = None\n",
    "    pd.set_option( 'display.expand_frame_repr', False )\n",
    "    \n",
    "    sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-23T02:14:35.736020Z",
     "start_time": "2023-04-23T02:14:35.736020Z"
    }
   },
   "outputs": [],
   "source": [
    "jupyter_settings()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## 0.2. Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-23T02:14:35.737019Z",
     "start_time": "2023-04-23T02:14:35.737019Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_sales_raw = pd.read_csv( '../data/train.csv', low_memory=False )\n",
    "df_store_raw = pd.read_csv( '../data/store.csv', low_memory=False )\n",
    "\n",
    "# merge\n",
    "df_raw = pd.merge( df_sales_raw, df_store_raw, how='left', on='Store' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.0. PASSO 01 - DESCRICAO DOS DADOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-23T02:14:35.739018Z",
     "start_time": "2023-04-23T02:14:35.739018Z"
    }
   },
   "outputs": [],
   "source": [
    "df1 = df_raw.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-10T11:26:41.736121Z",
     "start_time": "2019-11-10T11:26:41.732986Z"
    }
   },
   "source": [
    "## 1.1. Rename Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-23T02:14:35.741017Z",
     "start_time": "2023-04-23T02:14:35.741017Z"
    }
   },
   "outputs": [],
   "source": [
    "cols_old = ['Store', 'DayOfWeek', 'Date', 'Sales', 'Customers', 'Open', 'Promo', 'StateHoliday', 'SchoolHoliday', \n",
    "            'StoreType', 'Assortment', 'CompetitionDistance', 'CompetitionOpenSinceMonth',\n",
    "            'CompetitionOpenSinceYear', 'Promo2', 'Promo2SinceWeek', 'Promo2SinceYear', 'PromoInterval']\n",
    "\n",
    "snakecase = lambda x: inflection.underscore( x )\n",
    "\n",
    "cols_new = list( map( snakecase, cols_old ) )\n",
    "\n",
    "# rename\n",
    "df1.columns = cols_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-10T11:26:41.736121Z",
     "start_time": "2019-11-10T11:26:41.732986Z"
    }
   },
   "source": [
    "## 1.2. Data Dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-23T02:14:35.743016Z",
     "start_time": "2023-04-23T02:14:35.743016Z"
    }
   },
   "outputs": [],
   "source": [
    "print( 'Number of Rows: {}'.format( df1.shape[0] ) )\n",
    "print( 'Number of Cols: {}'.format( df1.shape[1] ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-10T11:26:41.736121Z",
     "start_time": "2019-11-10T11:26:41.732986Z"
    }
   },
   "source": [
    "## 1.3. Data Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-23T02:14:35.745015Z",
     "start_time": "2023-04-23T02:14:35.745015Z"
    }
   },
   "outputs": [],
   "source": [
    "df1['date'] = pd.to_datetime( df1['date'] )\n",
    "df1.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-10T11:26:41.736121Z",
     "start_time": "2019-11-10T11:26:41.732986Z"
    }
   },
   "source": [
    "## 1.4. Check NA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-23T02:14:35.746014Z",
     "start_time": "2023-04-23T02:14:35.746014Z"
    }
   },
   "outputs": [],
   "source": [
    "df1.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-10T11:26:41.736121Z",
     "start_time": "2019-11-10T11:26:41.732986Z"
    }
   },
   "source": [
    "## 1.5. Fillout NA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-23T02:14:35.748013Z",
     "start_time": "2023-04-23T02:14:35.748013Z"
    }
   },
   "outputs": [],
   "source": [
    "df1.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-23T02:14:35.751011Z",
     "start_time": "2023-04-23T02:14:35.751011Z"
    }
   },
   "outputs": [],
   "source": [
    "#competition_distance        \n",
    "df1['competition_distance'] = df1['competition_distance'].apply( lambda x: 200000.0 if math.isnan( x ) else x )\n",
    "\n",
    "#competition_open_since_month\n",
    "df1['competition_open_since_month'] = df1.apply( lambda x: x['date'].month if math.isnan( x['competition_open_since_month'] ) else x['competition_open_since_month'], axis=1 )\n",
    "\n",
    "#competition_open_since_year \n",
    "df1['competition_open_since_year'] = df1.apply( lambda x: x['date'].year if math.isnan( x['competition_open_since_year'] ) else x['competition_open_since_year'], axis=1 )\n",
    "\n",
    "#promo2_since_week           \n",
    "df1['promo2_since_week'] = df1.apply( lambda x: x['date'].week if math.isnan( x['promo2_since_week'] ) else x['promo2_since_week'], axis=1 )\n",
    "\n",
    "#promo2_since_year           \n",
    "df1['promo2_since_year'] = df1.apply( lambda x: x['date'].year if math.isnan( x['promo2_since_year'] ) else x['promo2_since_year'], axis=1 )\n",
    "\n",
    "#promo_interval              \n",
    "month_map = {1: 'Jan',  2: 'Fev',  3: 'Mar',  4: 'Apr',  5: 'May',  6: 'Jun',  7: 'Jul',  8: 'Aug',  9: 'Sep',  10: 'Oct', 11: 'Nov', 12: 'Dec'}\n",
    "\n",
    "df1['promo_interval'].fillna(0, inplace=True)\n",
    "\n",
    "df1['month_map'] = df1['date'].dt.month.map(month_map)\n",
    "\n",
    "df1['is_promo'] = df1[['promo_interval', 'month_map']].apply(lambda x: 0 if x['promo_interval'] == 0 else 1 if x['month_map'] in x['promo_interval'].split( ',' ) else 0, axis=1 )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-23T02:14:35.753010Z",
     "start_time": "2023-04-23T02:14:35.753010Z"
    }
   },
   "outputs": [],
   "source": [
    "df1.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-10T11:26:41.736121Z",
     "start_time": "2019-11-10T11:26:41.732986Z"
    },
    "heading_collapsed": true
   },
   "source": [
    "## 1.6. Change Data Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-23T02:14:35.754009Z",
     "start_time": "2023-04-23T02:14:35.754009Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# competiton\n",
    "df1['competition_open_since_month'] = df1['competition_open_since_month'].astype( int )\n",
    "df1['competition_open_since_year'] = df1['competition_open_since_year'].astype( int )\n",
    "    \n",
    "# promo2\n",
    "df1['promo2_since_week'] = df1['promo2_since_week'].astype( int )\n",
    "df1['promo2_since_year'] = df1['promo2_since_year'].astype( int )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-10T11:26:41.736121Z",
     "start_time": "2019-11-10T11:26:41.732986Z"
    }
   },
   "source": [
    "## 1.7. Descriptive Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-23T02:14:35.756008Z",
     "start_time": "2023-04-23T02:14:35.756008Z"
    }
   },
   "outputs": [],
   "source": [
    "num_attributes = df1.select_dtypes( include=['int64', 'float64'] )\n",
    "cat_attributes = df1.select_dtypes( exclude=['int64', 'float64', 'datetime64[ns]'] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-10T11:26:41.736121Z",
     "start_time": "2019-11-10T11:26:41.732986Z"
    }
   },
   "source": [
    "### 1.7.1. Numerical Atributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-23T02:14:35.758007Z",
     "start_time": "2023-04-23T02:14:35.758007Z"
    },
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "# Central Tendency - mean, meadina \n",
    "ct1 = pd.DataFrame( num_attributes.apply( np.mean ) ).T\n",
    "ct2 = pd.DataFrame( num_attributes.apply( np.median ) ).T\n",
    "\n",
    "# dispersion - std, min, max, range, skew, kurtosis\n",
    "d1 = pd.DataFrame( num_attributes.apply( np.std ) ).T \n",
    "d2 = pd.DataFrame( num_attributes.apply( min ) ).T \n",
    "d3 = pd.DataFrame( num_attributes.apply( max ) ).T \n",
    "d4 = pd.DataFrame( num_attributes.apply( lambda x: x.max() - x.min() ) ).T \n",
    "d5 = pd.DataFrame( num_attributes.apply( lambda x: x.skew() ) ).T \n",
    "d6 = pd.DataFrame( num_attributes.apply( lambda x: x.kurtosis() ) ).T \n",
    "\n",
    "# concatenar\n",
    "m = pd.concat( [d2, d3, d4, ct1, ct2, d1, d5, d6] ).T.reset_index()\n",
    "m.columns = ['attributes', 'min', 'max', 'range', 'mean', 'median', 'std', 'skew', 'kurtosis']\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-23T02:14:35.759006Z",
     "start_time": "2023-04-23T02:14:35.759006Z"
    }
   },
   "outputs": [],
   "source": [
    "sns.distplot( df1['competition_distance'], kde=False )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-10T11:26:41.736121Z",
     "start_time": "2019-11-10T11:26:41.732986Z"
    }
   },
   "source": [
    "### 1.7.2. Categorical Atributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-23T02:14:35.761005Z",
     "start_time": "2023-04-23T02:14:35.761005Z"
    }
   },
   "outputs": [],
   "source": [
    "cat_attributes.apply( lambda x: x.unique().shape[0] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-23T02:14:35.763005Z",
     "start_time": "2023-04-23T02:14:35.763005Z"
    }
   },
   "outputs": [],
   "source": [
    "aux = df1[(df1['state_holiday'] != '0') & (df1['sales'] > 0)]\n",
    "\n",
    "plt.subplot( 1, 3, 1 )\n",
    "sns.boxplot( x='state_holiday', y='sales', data=aux )\n",
    "\n",
    "plt.subplot( 1, 3, 2 )\n",
    "sns.boxplot( x='store_type', y='sales', data=aux )\n",
    "\n",
    "plt.subplot( 1, 3, 3 )\n",
    "sns.boxplot( x='assortment', y='sales', data=aux )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.0. PASSO 02 - FEATURE ENGINEERING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-23T02:14:35.765003Z",
     "start_time": "2023-04-23T02:14:35.765003Z"
    }
   },
   "outputs": [],
   "source": [
    "df2 = df1.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## 2.1. Mapa Mental de Hipoteses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-23T02:14:35.766003Z",
     "start_time": "2023-04-23T02:14:35.766003Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Image( 'img/MindMapHypothesis.png' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Criacao das Hipoteses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1. Hipoteses Loja"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.** Lojas com número maior de funcionários deveriam vender mais.\n",
    "\n",
    "**2.** Lojas com maior capacidade de estoque deveriam vender mais.\n",
    "\n",
    "**3.** Lojas com maior porte deveriam vender mais.\n",
    "\n",
    "**4.** Lojas com maior sortimentos deveriam vender mais.\n",
    "\n",
    "**5.** Lojas com competidores mais próximos deveriam vender menos.\n",
    "\n",
    "**6.** Lojas com competidores à mais tempo deveriam vendem mais."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2. Hipoteses Produto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-16T21:22:20.284469Z",
     "start_time": "2019-11-16T21:22:20.236577Z"
    }
   },
   "source": [
    "**1.** Lojas que investem mais em Marketing deveriam vender mais.\n",
    "\n",
    "**2.** Lojas com maior exposição de produto deveriam vender mais.\n",
    "\n",
    "**3.** Lojas com produtos com preço menor deveriam vender mais.\n",
    "\n",
    "**5.** Lojas com promoções mais agressivas ( descontos maiores ), deveriam vender mais.\n",
    "\n",
    "**6.** Lojas com promoções ativas por mais tempo deveriam vender mais.\n",
    "\n",
    "**7.** Lojas com mais dias de promoção deveriam vender mais.\n",
    "\n",
    "**8.** Lojas com mais promoções consecutivas deveriam vender mais."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.3. Hipoteses Tempo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-16T21:24:09.377189Z",
     "start_time": "2019-11-16T21:24:09.339135Z"
    }
   },
   "source": [
    "**1.** Lojas abertas durante o feriado de Natal deveriam vender mais.\n",
    "\n",
    "**2.** Lojas deveriam vender mais ao longo dos anos.\n",
    "\n",
    "**3.** Lojas deveriam vender mais no segundo semestre do ano.\n",
    "\n",
    "**4.** Lojas deveriam vender mais depois do dia 10 de cada mês.\n",
    "\n",
    "**5.** Lojas deveriam vender menos aos finais de semana.\n",
    "\n",
    "**6.** Lojas deveriam vender menos durante os feriados escolares."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. Lista Final de Hipóteses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.** Lojas com maior sortimentos deveriam vender mais.\n",
    "\n",
    "**2.** Lojas com competidores mais próximos deveriam vender menos.\n",
    "\n",
    "**3.** Lojas com competidores à mais tempo deveriam vendem mais."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.** Lojas com promoções ativas por mais tempo deveriam vender mais.\n",
    "\n",
    "**5.** Lojas com mais dias de promoção deveriam vender mais.\n",
    "\n",
    "**7.** Lojas com mais promoções consecutivas deveriam vender mais."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-16T21:33:04.092534Z",
     "start_time": "2019-11-16T21:33:04.074217Z"
    }
   },
   "source": [
    "**8.** Lojas abertas durante o feriado de Natal deveriam vender mais.\n",
    "\n",
    "**9.** Lojas deveriam vender mais ao longo dos anos.\n",
    "\n",
    "**10.** Lojas deveriam vender mais no segundo semestre do ano.\n",
    "\n",
    "**11.** Lojas deveriam vender mais depois do dia 10 de cada mês.\n",
    "\n",
    "**12.** Lojas deveriam vender menos aos finais de semana.\n",
    "\n",
    "**13.** Lojas deveriam vender menos durante os feriados escolares.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-23T02:14:35.768001Z",
     "start_time": "2023-04-23T02:14:35.768001Z"
    }
   },
   "outputs": [],
   "source": [
    "# year\n",
    "df2['year'] = df2['date'].dt.year\n",
    "\n",
    "# month\n",
    "df2['month'] = df2['date'].dt.month\n",
    "\n",
    "# day\n",
    "df2['day'] = df2['date'].dt.day\n",
    "\n",
    "# week of year\n",
    "df2['week_of_year'] = df2['date'].dt.weekofyear\n",
    "\n",
    "# year week\n",
    "df2['year_week'] = df2['date'].dt.strftime( '%Y-%W' )\n",
    "\n",
    "# competition since\n",
    "df2['competition_since'] = df2.apply( lambda x: datetime.datetime( year=x['competition_open_since_year'], month=x['competition_open_since_month'],day=1 ), axis=1 )\n",
    "df2['competition_time_month'] = ( ( df2['date'] - df2['competition_since'] )/30 ).apply( lambda x: x.days ).astype( int )\n",
    "\n",
    "# promo since\n",
    "df2['promo_since'] = df2['promo2_since_year'].astype( str ) + '-' + df2['promo2_since_week'].astype( str )\n",
    "df2['promo_since'] = df2['promo_since'].apply( lambda x: datetime.datetime.strptime( x + '-1', '%Y-%W-%w' ) - datetime.timedelta( days=7 ) )\n",
    "df2['promo_time_week'] = ( ( df2['date'] - df2['promo_since'] )/7 ).apply( lambda x: x.days ).astype( int )\n",
    "\n",
    "# assortment\n",
    "df2['assortment'] = df2['assortment'].apply( lambda x: 'basic' if x == 'a' else 'extra' if x == 'b' else 'extended' )\n",
    "\n",
    "# state holiday\n",
    "df2['state_holiday'] = df2['state_holiday'].apply( lambda x: 'public_holiday' if x == 'a' else 'easter_holiday' if x == 'b' else 'christmas' if x == 'c' else 'regular_day' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.0. PASSO 03 - FILTRAGEM DE VARIÁVEIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-23T02:14:35.770000Z",
     "start_time": "2023-04-23T02:14:35.770000Z"
    }
   },
   "outputs": [],
   "source": [
    "df3 = df2.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Filtragem das Linhas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-23T02:14:35.772000Z",
     "start_time": "2023-04-23T02:14:35.772000Z"
    }
   },
   "outputs": [],
   "source": [
    "df3 = df3[(df3['open'] != 0) & (df3['sales'] > 0)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Selecao das Colunas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-23T02:14:35.772998Z",
     "start_time": "2023-04-23T02:14:35.772998Z"
    }
   },
   "outputs": [],
   "source": [
    "cols_drop = ['customers', 'open', 'promo_interval', 'month_map']\n",
    "df3 = df3.drop( cols_drop, axis=1 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.0. PASSO 04 - ANALISE EXPLORATORIA DOS DADOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-23T02:14:35.774997Z",
     "start_time": "2023-04-23T02:14:35.774997Z"
    }
   },
   "outputs": [],
   "source": [
    "df4 = df3.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1. Analise Univariada"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hide_input": true
   },
   "source": [
    "### 4.1.1. Response Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-23T02:14:35.776996Z",
     "start_time": "2023-04-23T02:14:35.776996Z"
    },
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "sns.distplot( df4['sales'], kde=False  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.2. Numerical Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-23T02:14:35.778995Z",
     "start_time": "2023-04-23T02:14:35.778995Z"
    },
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "num_attributes.hist( bins=25 );"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hide_input": true
   },
   "source": [
    "### 4.1.3. Categorical Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-23T02:14:35.780994Z",
     "start_time": "2023-04-23T02:14:35.780994Z"
    },
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "# state_holiday\n",
    "plt.subplot( 3, 2, 1 )\n",
    "a = df4[df4['state_holiday'] != 'regular_day']\n",
    "sns.countplot( a['state_holiday'] )\n",
    "\n",
    "plt.subplot( 3, 2, 2 )\n",
    "sns.kdeplot( df4[df4['state_holiday'] == 'public_holiday']['sales'], label='public_holiday', shade=True )\n",
    "sns.kdeplot( df4[df4['state_holiday'] == 'easter_holiday']['sales'], label='easter_holiday', shade=True )\n",
    "sns.kdeplot( df4[df4['state_holiday'] == 'christmas']['sales'], label='christmas', shade=True )\n",
    "\n",
    "# store_type\n",
    "plt.subplot( 3, 2, 3 )\n",
    "sns.countplot( df4['store_type'] )\n",
    "\n",
    "plt.subplot( 3, 2, 4 )\n",
    "sns.kdeplot( df4[df4['store_type'] == 'a']['sales'], label='a', shade=True )\n",
    "sns.kdeplot( df4[df4['store_type'] == 'b']['sales'], label='b', shade=True )\n",
    "sns.kdeplot( df4[df4['store_type'] == 'c']['sales'], label='c', shade=True )\n",
    "sns.kdeplot( df4[df4['store_type'] == 'd']['sales'], label='d', shade=True )\n",
    "\n",
    "# assortment\n",
    "plt.subplot( 3, 2, 5 )\n",
    "sns.countplot( df4['assortment'] )\n",
    "\n",
    "plt.subplot( 3, 2, 6 )\n",
    "sns.kdeplot( df4[df4['assortment'] == 'extended']['sales'], label='extended', shade=True )\n",
    "sns.kdeplot( df4[df4['assortment'] == 'basic']['sales'], label='basic', shade=True )\n",
    "sns.kdeplot( df4[df4['assortment'] == 'extra']['sales'], label='extra', shade=True )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2. Analise Bivariada"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hide_input": true
   },
   "source": [
    "### **H1.** Lojas com maior sortimentos deveriam vender mais.\n",
    "**FALSA** Lojas com MAIOR SORTIMENTO vendem MENOS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-23T02:14:35.781993Z",
     "start_time": "2023-04-23T02:14:35.781993Z"
    },
    "hidden": true,
    "hide_input": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "aux1 = df4[['assortment', 'sales']].groupby( 'assortment' ).sum().reset_index()\n",
    "sns.barplot( x='assortment', y='sales', data=aux1 );\n",
    "\n",
    "aux2 = df4[['year_week', 'assortment', 'sales']].groupby( ['year_week','assortment'] ).sum().reset_index()\n",
    "aux2.pivot( index='year_week', columns='assortment', values='sales' ).plot()\n",
    "\n",
    "aux3 = aux2[aux2['assortment'] == 'extra']\n",
    "aux3.pivot( index='year_week', columns='assortment', values='sales' ).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### **H2.** Lojas com competidores mais próximos deveriam vender menos.\n",
    "**FALSA** Lojas com COMPETIDORES MAIS PROXIMOS vendem MAIS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-23T02:14:35.783992Z",
     "start_time": "2023-04-23T02:14:35.783992Z"
    },
    "hidden": true,
    "hide_input": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "aux1 = df4[['competition_distance', 'sales']].groupby( 'competition_distance' ).sum().reset_index()\n",
    "\n",
    "plt.subplot( 1, 3, 1 )\n",
    "sns.scatterplot( x ='competition_distance', y='sales', data=aux1 );\n",
    "\n",
    "plt.subplot( 1, 3, 2 )\n",
    "bins = list( np.arange( 0, 20000, 1000) )\n",
    "aux1['competition_distance_binned'] = pd.cut( aux1['competition_distance'], bins=bins )\n",
    "aux2 = aux1[['competition_distance_binned', 'sales']].groupby( 'competition_distance_binned' ).sum().reset_index()\n",
    "sns.barplot( x='competition_distance_binned', y='sales', data=aux2 );\n",
    "plt.xticks( rotation=90 );\n",
    "\n",
    "plt.subplot( 1, 3, 3 )\n",
    "x = sns.heatmap( aux1.corr( method='pearson' ), annot=True );\n",
    "bottom, top = x.get_ylim()\n",
    "x.set_ylim( bottom+0.5, top-0.5 );"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### **H3.** Lojas com competidores à mais tempo deveriam vendem mais.\n",
    "**FALSE** Lojas com COMPETIDORES À MAIS TEMPO vendem MENOS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-23T02:14:35.784991Z",
     "start_time": "2023-04-23T02:14:35.784991Z"
    },
    "hidden": true,
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "plt.subplot( 1, 3, 1 )\n",
    "aux1 = df4[['competition_time_month', 'sales']].groupby( 'competition_time_month' ).sum().reset_index()\n",
    "aux2 = aux1[( aux1['competition_time_month'] < 120 ) & ( aux1['competition_time_month'] != 0 )]\n",
    "sns.barplot( x='competition_time_month', y='sales', data=aux2 );\n",
    "plt.xticks( rotation=90 );\n",
    "\n",
    "plt.subplot( 1, 3, 2 )\n",
    "sns.regplot( x='competition_time_month', y='sales', data=aux2 );\n",
    "\n",
    "plt.subplot( 1, 3, 3 )\n",
    "x = sns.heatmap( aux1.corr( method='pearson'), annot=True );\n",
    "bottom, top = x.get_ylim()\n",
    "x.set_ylim( bottom+0.5, top-0.5);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### **H4.** Lojas com promoções ativas por mais tempo deveriam vender mais.\n",
    "**FALSA** Lojas com promocoes ativas por mais tempo vendem menos, depois de um certo periodo de promocao"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-23T02:14:35.787990Z",
     "start_time": "2023-04-23T02:14:35.787990Z"
    },
    "hidden": true,
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "aux1 = df4[['promo_time_week', 'sales']].groupby( 'promo_time_week').sum().reset_index()\n",
    "\n",
    "grid = GridSpec( 2, 3 )\n",
    "\n",
    "plt.subplot( grid[0,0] )\n",
    "aux2 = aux1[aux1['promo_time_week'] > 0] # promo extendido\n",
    "sns.barplot( x='promo_time_week', y='sales', data=aux2 );\n",
    "plt.xticks( rotation=90 );\n",
    "\n",
    "plt.subplot( grid[0,1] )\n",
    "sns.regplot( x='promo_time_week', y='sales', data=aux2 );\n",
    "\n",
    "plt.subplot( grid[1,0] )\n",
    "aux3 = aux1[aux1['promo_time_week'] < 0] # promo regular\n",
    "sns.barplot( x='promo_time_week', y='sales', data=aux3 );\n",
    "plt.xticks( rotation=90 );\n",
    "\n",
    "plt.subplot( grid[1,1] )\n",
    "sns.regplot( x='promo_time_week', y='sales', data=aux3 );\n",
    "\n",
    "plt.subplot( grid[:,2] )\n",
    "sns.heatmap( aux1.corr( method='pearson' ), annot=True );"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### <s>**H5.** Lojas com mais dias de promoção deveriam vender mais.</s>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### **H7.** Lojas com mais promoções consecutivas deveriam vender mais.\n",
    "**FALSA** Lojas com mais promocoes consecutivas vendem menos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-23T02:14:35.788989Z",
     "start_time": "2023-04-23T02:14:35.788989Z"
    },
    "hidden": true,
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "df4[['promo', 'promo2', 'sales']].groupby( ['promo', 'promo2'] ).sum().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-23T02:14:35.790988Z",
     "start_time": "2023-04-23T02:14:35.790988Z"
    },
    "hidden": true,
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "aux1 = df4[( df4['promo'] == 1 ) & ( df4['promo2'] == 1 )][['year_week', 'sales']].groupby( 'year_week' ).sum().reset_index()\n",
    "ax = aux1.plot()\n",
    "\n",
    "aux2 = df4[( df4['promo'] == 1 ) & ( df4['promo2'] == 0 )][['year_week', 'sales']].groupby( 'year_week' ).sum().reset_index()\n",
    "aux2.plot( ax=ax )\n",
    "\n",
    "ax.legend( labels=['Tradicional & Extendida', 'Extendida']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-16T21:33:04.092534Z",
     "start_time": "2019-11-16T21:33:04.074217Z"
    },
    "heading_collapsed": true
   },
   "source": [
    "### **H8.** Lojas abertas durante o feriado de Natal deveriam vender mais.\n",
    "**FALSA** Lojas abertas durante o feriado do Natal vendem menos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-23T02:14:35.792987Z",
     "start_time": "2023-04-23T02:14:35.792987Z"
    },
    "hidden": true,
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "aux = df4[df4['state_holiday'] != 'regular_day']\n",
    "\n",
    "plt.subplot( 1, 2, 1 )\n",
    "aux1 = aux[['state_holiday', 'sales']].groupby( 'state_holiday' ).sum().reset_index()\n",
    "sns.barplot( x='state_holiday', y='sales', data=aux1 );\n",
    "\n",
    "plt.subplot( 1, 2, 2 )\n",
    "aux2 = aux[['year', 'state_holiday', 'sales']].groupby( ['year', 'state_holiday'] ).sum().reset_index()\n",
    "sns.barplot( x='year', y='sales', hue='state_holiday', data=aux2 );"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-16T21:33:04.092534Z",
     "start_time": "2019-11-16T21:33:04.074217Z"
    },
    "heading_collapsed": true
   },
   "source": [
    "### **H9.** Lojas deveriam vender mais ao longo dos anos.\n",
    "**FALSA** Lojas vendem menos ao longo dos anos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-23T02:14:35.794986Z",
     "start_time": "2023-04-23T02:14:35.794986Z"
    },
    "hidden": true,
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "aux1 = df4[['year', 'sales']].groupby( 'year' ).sum().reset_index()\n",
    "\n",
    "plt.subplot( 1, 3, 1 )\n",
    "sns.barplot( x='year', y='sales', data=aux1 );\n",
    "\n",
    "plt.subplot( 1, 3, 2 )\n",
    "sns.regplot( x='year', y='sales', data=aux1 );\n",
    "\n",
    "plt.subplot( 1, 3, 3 )\n",
    "sns.heatmap( aux1.corr( method='pearson' ), annot=True );"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-16T21:33:04.092534Z",
     "start_time": "2019-11-16T21:33:04.074217Z"
    },
    "heading_collapsed": true
   },
   "source": [
    "### **H10.** Lojas deveriam vender mais no segundo semestre do ano.\n",
    "**FALSA** Lojas vendem menos no segundo semestre do ano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-23T02:14:35.796985Z",
     "start_time": "2023-04-23T02:14:35.795985Z"
    },
    "hidden": true,
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "aux1 = df4[['month', 'sales']].groupby( 'month' ).sum().reset_index()\n",
    "\n",
    "plt.subplot( 1, 3, 1 )\n",
    "sns.barplot( x='month', y='sales', data=aux1 );\n",
    "\n",
    "plt.subplot( 1, 3, 2 )\n",
    "sns.regplot( x='month', y='sales', data=aux1 );\n",
    "\n",
    "plt.subplot( 1, 3, 3 )\n",
    "sns.heatmap( aux1.corr( method='pearson' ), annot=True );"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-16T21:33:04.092534Z",
     "start_time": "2019-11-16T21:33:04.074217Z"
    },
    "heading_collapsed": true
   },
   "source": [
    "### **H11.** Lojas deveriam vender mais depois do dia 10 de cada mês.\n",
    "**VERDADEIRA** Lojas vendem mais depois do dia 10 de cada mes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-23T02:14:35.797984Z",
     "start_time": "2023-04-23T02:14:35.797984Z"
    },
    "hidden": true,
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "aux1 = df4[['day', 'sales']].groupby( 'day' ).sum().reset_index()\n",
    "\n",
    "plt.subplot( 2, 2, 1 )\n",
    "sns.barplot( x='day', y='sales', data=aux1 );\n",
    "\n",
    "plt.subplot( 2, 2, 2 )\n",
    "sns.regplot( x='day', y='sales', data=aux1 );\n",
    "\n",
    "plt.subplot( 2, 2, 3 )\n",
    "sns.heatmap( aux1.corr( method='pearson' ), annot=True );\n",
    "\n",
    "aux1['before_after'] = aux1['day'].apply( lambda x: 'before_10_days' if x <= 10 else 'after_10_days' )\n",
    "aux2 =aux1[['before_after', 'sales']].groupby( 'before_after' ).sum().reset_index()\n",
    "\n",
    "plt.subplot( 2, 2, 4 )\n",
    "sns.barplot( x='before_after', y='sales', data=aux2 );"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-16T21:33:04.092534Z",
     "start_time": "2019-11-16T21:33:04.074217Z"
    },
    "heading_collapsed": true
   },
   "source": [
    "### **H12.** Lojas deveriam vender menos aos finais de semana.\n",
    "**VERDADEIRA** Lojas vendem menos nos final de semana"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-23T02:14:35.799983Z",
     "start_time": "2023-04-23T02:14:35.799983Z"
    },
    "hidden": true,
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "aux1 = df4[['day_of_week', 'sales']].groupby( 'day_of_week' ).sum().reset_index()\n",
    "\n",
    "plt.subplot( 1, 3, 1 )\n",
    "sns.barplot( x='day_of_week', y='sales', data=aux1 );\n",
    "\n",
    "plt.subplot( 1, 3, 2 )\n",
    "sns.regplot( x='day_of_week', y='sales', data=aux1 );\n",
    "\n",
    "plt.subplot( 1, 3, 3 )\n",
    "sns.heatmap( aux1.corr( method='pearson' ), annot=True );"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-16T21:33:04.092534Z",
     "start_time": "2019-11-16T21:33:04.074217Z"
    },
    "heading_collapsed": true
   },
   "source": [
    "### **H13.** Lojas deveriam vender menos durante os feriados escolares.\n",
    "**VERDADEIRA** Lojas vendem menos durante os feriadso escolares, except os meses de Julho e Agosto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-23T02:14:35.801982Z",
     "start_time": "2023-04-23T02:14:35.801982Z"
    },
    "hidden": true,
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "aux1 = df4[['school_holiday', 'sales']].groupby( 'school_holiday' ).sum().reset_index()\n",
    "plt.subplot( 2, 1, 1 )\n",
    "sns.barplot( x='school_holiday', y='sales', data=aux1 );\n",
    "\n",
    "aux2 = df4[['month', 'school_holiday', 'sales']].groupby( ['month','school_holiday'] ).sum().reset_index()\n",
    "plt.subplot( 2, 1, 2 )\n",
    "sns.barplot( x='month', y='sales', hue='school_holiday', data=aux2 );"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.1. Resumo das Hipoteses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-23T02:14:35.803981Z",
     "start_time": "2023-04-23T02:14:35.803981Z"
    },
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "from tabulate import tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-23T02:14:35.805980Z",
     "start_time": "2023-04-23T02:14:35.805980Z"
    },
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "tab =[['Hipoteses', 'Conclusao', 'Relevancia'],\n",
    "      ['H1', 'Falsa', 'Baixa'],  \n",
    "      ['H2', 'Falsa', 'Media'],  \n",
    "      ['H3', 'Falsa', 'Media'],\n",
    "      ['H4', 'Falsa', 'Baixa'],\n",
    "      ['H5', '-', '-'],\n",
    "      ['H7', 'Falsa', 'Baixa'],\n",
    "      ['H8', 'Falsa', 'Media'],\n",
    "      ['H9', 'Falsa', 'Alta'],\n",
    "      ['H10', 'Falsa', 'Alta'],\n",
    "      ['H11', 'Verdadeira', 'Alta'],\n",
    "      ['H12', 'Verdadeira', 'Alta'],\n",
    "      ['H13', 'Verdadeira', 'Baixa'],\n",
    "     ]  \n",
    "print( tabulate( tab, headers='firstrow' ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3. Analise Multivariada"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.1. Numerical Attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-23T02:14:35.807979Z",
     "start_time": "2023-04-23T02:14:35.807979Z"
    },
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "correlation = num_attributes.corr( method='pearson' )\n",
    "sns.heatmap( correlation, annot=True );"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.2. Categorical Attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-23T02:14:35.808978Z",
     "start_time": "2023-04-23T02:14:35.808978Z"
    },
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "# only categorical data\n",
    "a = df4.select_dtypes( include='object' )\n",
    "\n",
    "# Calculate cramer V\n",
    "a1 = cramer_v( a['state_holiday'], a['state_holiday'] )\n",
    "a2 = cramer_v( a['state_holiday'], a['store_type'] )\n",
    "a3 = cramer_v( a['state_holiday'], a['assortment'] )\n",
    "\n",
    "a4 = cramer_v( a['store_type'], a['state_holiday'] )\n",
    "a5 = cramer_v( a['store_type'], a['store_type'] )\n",
    "a6 = cramer_v( a['store_type'], a['assortment'] )\n",
    "\n",
    "a7 = cramer_v( a['assortment'], a['state_holiday'] )\n",
    "a8 = cramer_v( a['assortment'], a['store_type'] )\n",
    "a9 = cramer_v( a['assortment'], a['assortment'] )\n",
    "\n",
    "# Final dataset\n",
    "d = pd.DataFrame( {'state_holiday': [a1, a2, a3], \n",
    "               'store_type': [a4, a5, a6],\n",
    "               'assortment': [a7, a8, a9]  })\n",
    "d = d.set_index( d.columns )\n",
    "\n",
    "sns.heatmap( d, annot=True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.0. PASSO 05 - DATA PREPARATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-23T02:14:35.810977Z",
     "start_time": "2023-04-23T02:14:35.810977Z"
    }
   },
   "outputs": [],
   "source": [
    "df5 = df4.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## 5.1. Normalizacao"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2. Rescaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-23T02:14:35.812976Z",
     "start_time": "2023-04-23T02:14:35.812976Z"
    }
   },
   "outputs": [],
   "source": [
    "rs = RobustScaler()\n",
    "mms = MinMaxScaler()\n",
    "\n",
    "# competition distance\n",
    "df5['competition_distance'] = rs.fit_transform( df5[['competition_distance']].values )\n",
    "pickle.dump( rs, open( 'parameter/competition_distance_scaler.pkl', 'wb') )\n",
    "\n",
    "# competition time month\n",
    "df5['competition_time_month'] = rs.fit_transform( df5[['competition_time_month']].values )\n",
    "pickle.dump( rs, open( 'parameter/competition_time_month_scaler.pkl', 'wb') )\n",
    "\n",
    "# promo time week\n",
    "df5['promo_time_week'] = mms.fit_transform( df5[['promo_time_week']].values )\n",
    "pickle.dump( rs, open( 'parameter/promo_time_week_scaler.pkl', 'wb') )\n",
    "\n",
    "# year\n",
    "df5['year'] = mms.fit_transform( df5[['year']].values )\n",
    "pickle.dump( mms, open( 'parameter/year_scaler.pkl', 'wb') )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3. Transformacao"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3.1. Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-23T02:14:35.814975Z",
     "start_time": "2023-04-23T02:14:35.814975Z"
    }
   },
   "outputs": [],
   "source": [
    "# state_holiday - One Hot Encoding\n",
    "df5 = pd.get_dummies( df5, prefix=['state_holiday'], columns=['state_holiday'] )\n",
    "\n",
    "# store_type - Label Encoding\n",
    "le = LabelEncoder()\n",
    "df5['store_type'] = le.fit_transform( df5['store_type'] )\n",
    "pickle.dump( le, open( 'parameter/store_type_scaler.pkl', 'wb') )\n",
    "\n",
    "# assortment - Ordinal Encoding\n",
    "assortment_dict = {'basic': 1,  'extra': 2, 'extended': 3}\n",
    "df5['assortment'] = df5['assortment'].map( assortment_dict )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3.2. Response Variable Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-23T02:14:35.816973Z",
     "start_time": "2023-04-23T02:14:35.816973Z"
    }
   },
   "outputs": [],
   "source": [
    "df5['sales'] = np.log1p( df5['sales'] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3.3. Nature Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-23T02:14:35.818972Z",
     "start_time": "2023-04-23T02:14:35.818972Z"
    }
   },
   "outputs": [],
   "source": [
    "# day of week\n",
    "df5['day_of_week_sin'] = df5['day_of_week'].apply( lambda x: np.sin( x * ( 2. * np.pi/7 ) ) )\n",
    "df5['day_of_week_cos'] = df5['day_of_week'].apply( lambda x: np.cos( x * ( 2. * np.pi/7 ) ) )\n",
    "\n",
    "# month\n",
    "df5['month_sin'] = df5['month'].apply( lambda x: np.sin( x * ( 2. * np.pi/12 ) ) )\n",
    "df5['month_cos'] = df5['month'].apply( lambda x: np.cos( x * ( 2. * np.pi/12 ) ) )\n",
    "\n",
    "# day \n",
    "df5['day_sin'] = df5['day'].apply( lambda x: np.sin( x * ( 2. * np.pi/30 ) ) )\n",
    "df5['day_cos'] = df5['day'].apply( lambda x: np.cos( x * ( 2. * np.pi/30 ) ) )\n",
    "\n",
    "# week of year\n",
    "df5['week_of_year_sin'] = df5['week_of_year'].apply( lambda x: np.sin( x * ( 2. * np.pi/52 ) ) )\n",
    "df5['week_of_year_cos'] = df5['week_of_year'].apply( lambda x: np.cos( x * ( 2. * np.pi/52 ) ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.0. PASSO 06 - FEATURE SELECTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-23T02:14:35.819971Z",
     "start_time": "2023-04-23T02:14:35.819971Z"
    }
   },
   "outputs": [],
   "source": [
    "df6 = df5.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1. Split dataframe into training and test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-23T02:14:35.821970Z",
     "start_time": "2023-04-23T02:14:35.821970Z"
    }
   },
   "outputs": [],
   "source": [
    "cols_drop = ['week_of_year', 'day', 'month', 'day_of_week', 'promo_since', 'competition_since', 'year_week' ]\n",
    "df6 = df6.drop( cols_drop, axis=1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-23T02:14:35.823970Z",
     "start_time": "2023-04-23T02:14:35.823970Z"
    }
   },
   "outputs": [],
   "source": [
    "# training dataset\n",
    "X_train = df6[df6['date'] < '2015-06-19']\n",
    "y_train = X_train['sales']\n",
    "\n",
    "# test dataset\n",
    "X_test = df6[df6['date'] >= '2015-06-19']\n",
    "y_test = X_test['sales']\n",
    "\n",
    "print( 'Training Min Date: {}'.format( X_train['date'].min() ) )\n",
    "print( 'Training Max Date: {}'.format( X_train['date'].max() ) )\n",
    "\n",
    "print( '\\nTest Min Date: {}'.format( X_test['date'].min() ) )\n",
    "print( 'Test Max Date: {}'.format( X_test['date'].max() ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2. Boruta as Feature Selector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-23T02:14:35.825968Z",
     "start_time": "2023-04-23T02:14:35.825968Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "## training and test dataset for Boruta\n",
    "#X_train_n = X_train.drop( ['date', 'sales'], axis=1 ).values\n",
    "#y_train_n = y_train.values.ravel()\n",
    "#\n",
    "## define RandomForestRegressor\n",
    "#rf = RandomForestRegressor( n_jobs=-1 )\n",
    "#\n",
    "## define Boruta\n",
    "#boruta = BorutaPy( rf, n_estimators='auto', verbose=2, random_state=42 ).fit( X_train_n, y_train_n )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### 6.2.1. Best Features from Boruta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-23T02:14:35.827967Z",
     "start_time": "2023-04-23T02:14:35.827967Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#cols_selected = boruta.support_.tolist()\n",
    "#\n",
    "## best features\n",
    "#X_train_fs = X_train.drop( ['date', 'sales'], axis=1 )\n",
    "#cols_selected_boruta = X_train_fs.iloc[:, cols_selected].columns.to_list()\n",
    "#\n",
    "## not selected boruta\n",
    "#cols_not_selected_boruta = list( np.setdiff1d( X_train_fs.columns, cols_selected_boruta ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## 6.3. Manual Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-23T02:14:35.829966Z",
     "start_time": "2023-04-23T02:14:35.829966Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "cols_selected_boruta = [\n",
    "    'store',\n",
    "    'promo',\n",
    "    'store_type',\n",
    "    'assortment',\n",
    "    'competition_distance',\n",
    "    'competition_open_since_month',\n",
    "    'competition_open_since_year',\n",
    "    'promo2',\n",
    "    'promo2_since_week',\n",
    "    'promo2_since_year',\n",
    "    'competition_time_month',\n",
    "    'promo_time_week',\n",
    "    'day_of_week_sin',\n",
    "    'day_of_week_cos',\n",
    "    'month_sin',\n",
    "    'month_cos',\n",
    "    'day_sin',\n",
    "    'day_cos',\n",
    "    'week_of_year_sin',\n",
    "    'week_of_year_cos']\n",
    "\n",
    "# columns to add\n",
    "feat_to_add = ['date', 'sales']\n",
    "\n",
    "cols_selected_boruta_full = cols_selected_boruta.copy()\n",
    "cols_selected_boruta_full.extend( feat_to_add )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# 7.0. PASSO 07 - MACHINE LEARNING MODELLING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-23T02:14:35.830966Z",
     "start_time": "2023-04-23T02:14:35.830966Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "x_train = X_train[ cols_selected_boruta ]\n",
    "x_test = X_test[ cols_selected_boruta ]\n",
    "\n",
    "# Time Series Data Preparation\n",
    "x_training = X_train[ cols_selected_boruta_full ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## 7.1. Average Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-23T02:14:35.832964Z",
     "start_time": "2023-04-23T02:14:35.832964Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "aux1 = x_test.copy()\n",
    "aux1['sales'] = y_test.copy()\n",
    "\n",
    "# prediction\n",
    "aux2 = aux1[['store', 'sales']].groupby( 'store' ).mean().reset_index().rename( columns={'sales': 'predictions'} )\n",
    "aux1 = pd.merge( aux1, aux2, how='left', on='store' )\n",
    "yhat_baseline = aux1['predictions']\n",
    "\n",
    "# performance\n",
    "baseline_result = ml_error( 'Average Model', np.expm1( y_test ), np.expm1( yhat_baseline ) )\n",
    "baseline_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## 7.2. Linear Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-23T02:14:35.834964Z",
     "start_time": "2023-04-23T02:14:35.834964Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# model\n",
    "lr = LinearRegression().fit( x_train, y_train )\n",
    "\n",
    "# prediction\n",
    "yhat_lr = lr.predict( x_test )\n",
    "\n",
    "# performance\n",
    "lr_result = ml_error( 'Linear Regression', np.expm1( y_test ), np.expm1( yhat_lr ) )\n",
    "lr_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### 7.2.1. Linear Regression Model - Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-23T02:14:35.836962Z",
     "start_time": "2023-04-23T02:14:35.836962Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "lr_result_cv = cross_validation( x_training, 5, 'Linear Regression', lr, verbose=False )\n",
    "lr_result_cv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## 7.3. Linear Regression Regularized Model - Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-23T02:14:35.837961Z",
     "start_time": "2023-04-23T02:14:35.837961Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# model\n",
    "lrr = Lasso( alpha=0.01 ).fit( x_train, y_train )\n",
    "\n",
    "# prediction\n",
    "yhat_lrr = lrr.predict( x_test )\n",
    "\n",
    "# performance\n",
    "lrr_result = ml_error( 'Linear Regression - Lasso', np.expm1( y_test ), np.expm1( yhat_lrr ) )\n",
    "lrr_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### 7.3.1.  Lasso - Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-23T02:14:35.839960Z",
     "start_time": "2023-04-23T02:14:35.839960Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "lrr_result_cv = cross_validation( x_training, 5, 'Lasso', lrr, verbose=False )\n",
    "lrr_result_cv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## 7.4. Random Forest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-23T02:14:35.841959Z",
     "start_time": "2023-04-23T02:14:35.841959Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# model\n",
    "rf = RandomForestRegressor( n_estimators=100, n_jobs=-1, random_state=42 ).fit( x_train, y_train )\n",
    "\n",
    "# prediction\n",
    "yhat_rf = rf.predict( x_test )\n",
    "\n",
    "# performance\n",
    "rf_result = ml_error( 'Random Forest Regressor', np.expm1( y_test ), np.expm1( yhat_rf ) )\n",
    "rf_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### 7.4.1.  Random Forest Regressor - Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-23T02:14:35.843958Z",
     "start_time": "2023-04-23T02:14:35.843958Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "rf_result_cv = cross_validation( x_training, 5, 'Random Forest Regressor', rf, verbose=True )\n",
    "rf_result_cv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## 7.5. XGBoost Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-23T02:14:35.844957Z",
     "start_time": "2023-04-23T02:14:35.844957Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# model\n",
    "model_xgb = xgb.XGBRegressor( objective='reg:squarederror',\n",
    "                              n_estimators=100, \n",
    "                              eta=0.01, \n",
    "                              max_depth=10, \n",
    "                              subsample=0.7,\n",
    "                              colsample_bytee=0.9 ).fit( x_train, y_train )\n",
    "\n",
    "# prediction\n",
    "yhat_xgb = model_xgb.predict( x_test )\n",
    "\n",
    "# performance\n",
    "xgb_result = ml_error( 'XGBoost Regressor', np.expm1( y_test ), np.expm1( yhat_xgb ) )\n",
    "xgb_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### 7.5.1. XGBoost Regressor - Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-23T02:14:35.846956Z",
     "start_time": "2023-04-23T02:14:35.846956Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "xgb_result_cv = cross_validation( x_training, 5, 'XGBoost Regressor', model_xgb, verbose=True )\n",
    "xgb_result_cv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## 7.6. Compare Model's Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### 7.6.1. Single Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-23T02:14:35.848955Z",
     "start_time": "2023-04-23T02:14:35.848955Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "modelling_result = pd.concat( [baseline_result, lr_result, lrr_result, rf_result, xgb_result] )\n",
    "modelling_result.sort_values( 'RMSE' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### 7.6.2. Real Performance - Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-23T02:14:35.850954Z",
     "start_time": "2023-04-23T02:14:35.850954Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "modelling_result_cv = pd.concat( [lr_result_cv, lrr_result_cv, rf_result_cv, xgb_result_cv] )\n",
    "modelling_result_cv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8.0. PASSO 08 - HYPERPARAMETER FINE TUNING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.1. Random Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-23T02:14:35.851953Z",
     "start_time": "2023-04-23T02:14:35.851953Z"
    }
   },
   "outputs": [],
   "source": [
    "# param = {\n",
    "#    'n_estimators': [1500, 1700, 2500, 3000, 3500],\n",
    "#    'eta': [0.01, 0.03],\n",
    "#    'max_depth': [3, 5, 9],\n",
    "#    'subsample': [0.1, 0.5, 0.7],\n",
    "#    'colsample_bytree': [0.3, 0.7, 0.9],\n",
    "#    'min_child_weight': [3, 8, 15]\n",
    "#        }\n",
    "\n",
    "# MAX_EVAL = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-23T02:14:35.853952Z",
     "start_time": "2023-04-23T02:14:35.853952Z"
    }
   },
   "outputs": [],
   "source": [
    "#final_result = pd.DataFrame()\n",
    "#\n",
    "#for i in range( MAX_EVAL ):\n",
    "#    # choose values for parameters randomly\n",
    "#    hp = { k: random.sample( v, 1 )[0] for k, v in param.items() }\n",
    "#    print( hp )\n",
    "#    \n",
    "#    # model\n",
    "#    model_xgb = xgb.XGBRegressor( objective='reg:squarederror',\n",
    "#                                  n_estimators=hp['n_estimators'], \n",
    "#                                  eta=hp['eta'], \n",
    "#                                  max_depth=hp['max_depth'], \n",
    "#                                  subsample=hp['subsample'],\n",
    "#                                  colsample_bytee=hp['colsample_bytree'],\n",
    "#                                  min_child_weight=hp['min_child_weight'] )\n",
    "#\n",
    "#    # performance\n",
    "#    result = cross_validation( x_training, 5, 'XGBoost Regressor', model_xgb, verbose=True )\n",
    "#    final_result = pd.concat( [final_result, result] )\n",
    "#        \n",
    "#final_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-23T02:14:35.855951Z",
     "start_time": "2023-04-23T02:14:35.855951Z"
    }
   },
   "outputs": [],
   "source": [
    "#final_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.2. Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-23T02:14:35.856950Z",
     "start_time": "2023-04-23T02:14:35.856950Z"
    }
   },
   "outputs": [],
   "source": [
    "param_tuned = {\n",
    "    'n_estimators': 3000,\n",
    "    'eta': 0.03,\n",
    "    'max_depth': 5,\n",
    "    'subsample': 0.7,\n",
    "    'colsample_bytree': 0.7,\n",
    "    'min_child_weight': 3 \n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-23T02:14:35.858949Z",
     "start_time": "2023-04-23T02:14:35.858949Z"
    }
   },
   "outputs": [],
   "source": [
    "# model\n",
    "model_xgb_tuned = xgb.XGBRegressor( objective='reg:squarederror',\n",
    "                                    n_estimators=param_tuned['n_estimators'], \n",
    "                                    eta=param_tuned['eta'], \n",
    "                                    max_depth=param_tuned['max_depth'], \n",
    "                                    subsample=param_tuned['subsample'],\n",
    "                                    colsample_bytee=param_tuned['colsample_bytree'],\n",
    "                                    min_child_weight=param_tuned['min_child_weight'] ).fit( x_train, y_train )\n",
    "\n",
    "# prediction\n",
    "yhat_xgb_tuned = model_xgb_tuned.predict( x_test )\n",
    "\n",
    "# performance\n",
    "xgb_result_tuned = ml_error( 'XGBoost Regressor', np.expm1( y_test ), np.expm1( yhat_xgb_tuned ) )\n",
    "xgb_result_tuned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-23T02:14:35.860948Z",
     "start_time": "2023-04-23T02:14:35.860948Z"
    }
   },
   "outputs": [],
   "source": [
    "mpe = mean_percentage_error( np.expm1( y_test ), np.expm1( yhat_xgb_tuned ) )\n",
    "mpe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9.0. PASSO 09 - TRADUCAO E INTERPRETACAO DO ERRO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-23T02:14:35.861947Z",
     "start_time": "2023-04-23T02:14:35.861947Z"
    }
   },
   "outputs": [],
   "source": [
    "df9 = X_test[ cols_selected_boruta_full ]\n",
    "\n",
    "# rescale\n",
    "df9['sales'] = np.expm1( df9['sales'] )\n",
    "df9['predictions'] = np.expm1( yhat_xgb_tuned )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.1. Business Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-23T02:14:35.863946Z",
     "start_time": "2023-04-23T02:14:35.863946Z"
    }
   },
   "outputs": [],
   "source": [
    "# sum of predictions\n",
    "df91 = df9[['store', 'predictions']].groupby( 'store' ).sum().reset_index()\n",
    "\n",
    "# MAE and MAPE\n",
    "df9_aux1 = df9[['store', 'sales', 'predictions']].groupby( 'store' ).apply( lambda x: mean_absolute_error( x['sales'], x['predictions'] ) ).reset_index().rename( columns={0:'MAE'})\n",
    "df9_aux2 = df9[['store', 'sales', 'predictions']].groupby( 'store' ).apply( lambda x: mean_absolute_percentage_error( x['sales'], x['predictions'] ) ).reset_index().rename( columns={0:'MAPE'})\n",
    "\n",
    "# Merge\n",
    "df9_aux3 = pd.merge( df9_aux1, df9_aux2, how='inner', on='store' )\n",
    "df92 = pd.merge( df91, df9_aux3, how='inner', on='store' )\n",
    "\n",
    "# Scenarios\n",
    "df92['worst_scenario'] = df92['predictions'] - df92['MAE']\n",
    "df92['best_scenario'] = df92['predictions'] + df92['MAE']\n",
    "\n",
    "# order columns\n",
    "df92 = df92[['store', 'predictions', 'worst_scenario', 'best_scenario', 'MAE', 'MAPE']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-23T02:14:35.864946Z",
     "start_time": "2023-04-23T02:14:35.864946Z"
    }
   },
   "outputs": [],
   "source": [
    "df92.sort_values( 'MAPE', ascending=False ).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-23T02:14:35.866944Z",
     "start_time": "2023-04-23T02:14:35.866944Z"
    }
   },
   "outputs": [],
   "source": [
    "sns.scatterplot( x='store', y='MAPE', data=df92 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## 9.2. Total Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-23T02:14:35.868943Z",
     "start_time": "2023-04-23T02:14:35.868943Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df93 = df92[['predictions', 'worst_scenario', 'best_scenario']].apply( lambda x: np.sum( x ), axis=0 ).reset_index().rename( columns={'index': 'Scenario', 0:'Values'} )\n",
    "df93['Values'] = df93['Values'].map( 'R${:,.2f}'.format ) # map: mapeia o formato de reais para todas as linhas\n",
    "df93"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## 9.3. Machine Learning Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-23T02:14:35.870942Z",
     "start_time": "2023-04-23T02:14:35.870942Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df9['error'] = df9['sales'] - df9['predictions']\n",
    "df9['error_rate'] = df9['predictions'] / df9['sales']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-23T02:14:35.872942Z",
     "start_time": "2023-04-23T02:14:35.872942Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plt.subplot( 2, 2, 1 )\n",
    "sns.lineplot( x='date', y='sales', data=df9, label='SALES' )\n",
    "sns.lineplot( x='date', y='predictions', data=df9, label='PREDICTIONS' )\n",
    "\n",
    "plt.subplot( 2, 2, 2 )\n",
    "sns.lineplot( x='date', y='error_rate', data=df9 )\n",
    "plt.axhline( 1, linestyle='--')\n",
    "\n",
    "plt.subplot( 2, 2, 3 )\n",
    "sns.distplot( df9['error'] )\n",
    "\n",
    "plt.subplot( 2, 2, 4 )\n",
    "sns.scatterplot( df9['predictions'], df9['error'] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10.0. PASSO 10 - DEPLOY MODEL TO PRODUCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-23T02:14:35.874940Z",
     "start_time": "2023-04-23T02:14:35.874940Z"
    }
   },
   "outputs": [],
   "source": [
    "# Save Trained Model\n",
    "pickle.dump( model_xgb_tuned, open( '/Users/meigarom/repos/DataScience_Em_Producao/model/model_rossmann.pkl', 'wb' ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.1. Rossmann Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-23T02:14:35.876939Z",
     "start_time": "2023-04-23T02:14:35.876939Z"
    }
   },
   "outputs": [],
   "source": [
    "# anotações da classe rossmann:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-23T02:14:35.878938Z",
     "start_time": "2023-04-23T02:14:35.878938Z"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import inflection\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import datetime\n",
    "\n",
    "class Rossmann( object ):\n",
    "    def __init__( self ):\n",
    "        self.home_path='/Users/meigarom/repos/DataScience_Em_Producao/' \n",
    "        self.competition_distance_scaler   = pickle.load( open( self.home_path + 'parameter/competition_distance_scaler.pkl', 'rb') )\n",
    "        self.competition_time_month_scaler = pickle.load( open( self.home_path + 'parameter/competition_time_month_scaler.pkl', 'rb') )\n",
    "        self.promo_time_week_scaler        = pickle.load( open( self.home_path + 'parameter/promo_time_week_scaler.pkl', 'rb') )\n",
    "        self.year_scaler                   = pickle.load( open( self.home_path + 'parameter/year_scaler.pkl', 'rb') )\n",
    "        self.store_type_scaler             = pickle.load( open( self.home_path + 'parameter/store_type_scaler.pkl', 'rb') )\n",
    "        \n",
    "        \n",
    "    def data_cleaning( self, df1 ): \n",
    "        \n",
    "        ## 1.1. Rename Columns\n",
    "        cols_old = ['Store', 'DayOfWeek', 'Date', 'Open', 'Promo', 'StateHoliday', 'SchoolHoliday', \n",
    "                    'StoreType', 'Assortment', 'CompetitionDistance', 'CompetitionOpenSinceMonth',\n",
    "                    'CompetitionOpenSinceYear', 'Promo2', 'Promo2SinceWeek', 'Promo2SinceYear', 'PromoInterval']\n",
    "\n",
    "        snakecase = lambda x: inflection.underscore( x )\n",
    "\n",
    "        cols_new = list( map( snakecase, cols_old ) )\n",
    "\n",
    "        # rename\n",
    "        df1.columns = cols_new\n",
    "\n",
    "        ## 1.3. Data Types\n",
    "        df1['date'] = pd.to_datetime( df1['date'] )\n",
    "\n",
    "        ## 1.5. Fillout NA\n",
    "        #competition_distance        \n",
    "        df1['competition_distance'] = df1['competition_distance'].apply( lambda x: 200000.0 if math.isnan( x ) else x )\n",
    "\n",
    "        #competition_open_since_month\n",
    "        df1['competition_open_since_month'] = df1.apply( lambda x: x['date'].month if math.isnan( x['competition_open_since_month'] ) else x['competition_open_since_month'], axis=1 )\n",
    "\n",
    "        #competition_open_since_year \n",
    "        df1['competition_open_since_year'] = df1.apply( lambda x: x['date'].year if math.isnan( x['competition_open_since_year'] ) else x['competition_open_since_year'], axis=1 )\n",
    "\n",
    "        #promo2_since_week           \n",
    "        df1['promo2_since_week'] = df1.apply( lambda x: x['date'].week if math.isnan( x['promo2_since_week'] ) else x['promo2_since_week'], axis=1 )\n",
    "\n",
    "        #promo2_since_year           \n",
    "        df1['promo2_since_year'] = df1.apply( lambda x: x['date'].year if math.isnan( x['promo2_since_year'] ) else x['promo2_since_year'], axis=1 )\n",
    "\n",
    "        #promo_interval              \n",
    "        month_map = {1: 'Jan',  2: 'Fev',  3: 'Mar',  4: 'Apr',  5: 'May',  6: 'Jun',  7: 'Jul',  8: 'Aug',  9: 'Sep',  10: 'Oct', 11: 'Nov', 12: 'Dec'}\n",
    "\n",
    "        df1['promo_interval'].fillna(0, inplace=True )\n",
    "\n",
    "        df1['month_map'] = df1['date'].dt.month.map( month_map )\n",
    "\n",
    "        df1['is_promo'] = df1[['promo_interval', 'month_map']].apply( lambda x: 0 if x['promo_interval'] == 0 else 1 if x['month_map'] in x['promo_interval'].split( ',' ) else 0, axis=1 )\n",
    "\n",
    "        ## 1.6. Change Data Types\n",
    "        # competiton\n",
    "        df1['competition_open_since_month'] = df1['competition_open_since_month'].astype( int )\n",
    "        df1['competition_open_since_year'] = df1['competition_open_since_year'].astype( int )\n",
    "\n",
    "        # promo2\n",
    "        df1['promo2_since_week'] = df1['promo2_since_week'].astype( int )\n",
    "        df1['promo2_since_year'] = df1['promo2_since_year'].astype( int )\n",
    "        \n",
    "        return df1 \n",
    "\n",
    "\n",
    "    def feature_engineering( self, df2 ):\n",
    "\n",
    "        # year\n",
    "        df2['year'] = df2['date'].dt.year\n",
    "\n",
    "        # month\n",
    "        df2['month'] = df2['date'].dt.month\n",
    "\n",
    "        # day\n",
    "        df2['day'] = df2['date'].dt.day\n",
    "\n",
    "        # week of year\n",
    "        df2['week_of_year'] = df2['date'].dt.weekofyear\n",
    "\n",
    "        # year week\n",
    "        df2['year_week'] = df2['date'].dt.strftime( '%Y-%W' )\n",
    "\n",
    "        # competition since\n",
    "        df2['competition_since'] = df2.apply( lambda x: datetime.datetime( year=x['competition_open_since_year'], month=x['competition_open_since_month'],day=1 ), axis=1 )\n",
    "        df2['competition_time_month'] = ( ( df2['date'] - df2['competition_since'] )/30 ).apply( lambda x: x.days ).astype( int )\n",
    "\n",
    "        # promo since\n",
    "        df2['promo_since'] = df2['promo2_since_year'].astype( str ) + '-' + df2['promo2_since_week'].astype( str )\n",
    "        df2['promo_since'] = df2['promo_since'].apply( lambda x: datetime.datetime.strptime( x + '-1', '%Y-%W-%w' ) - datetime.timedelta( days=7 ) )\n",
    "        df2['promo_time_week'] = ( ( df2['date'] - df2['promo_since'] )/7 ).apply( lambda x: x.days ).astype( int )\n",
    "\n",
    "        # assortment\n",
    "        df2['assortment'] = df2['assortment'].apply( lambda x: 'basic' if x == 'a' else 'extra' if x == 'b' else 'extended' )\n",
    "\n",
    "        # state holiday\n",
    "        df2['state_holiday'] = df2['state_holiday'].apply( lambda x: 'public_holiday' if x == 'a' else 'easter_holiday' if x == 'b' else 'christmas' if x == 'c' else 'regular_day' )\n",
    "\n",
    "        # 3.0. PASSO 03 - FILTRAGEM DE VARIÁVEIS\n",
    "        ## 3.1. Filtragem das Linhas\n",
    "        df2 = df2[df2['open'] != 0]\n",
    "\n",
    "        ## 3.2. Selecao das Colunas\n",
    "        cols_drop = ['open', 'promo_interval', 'month_map']\n",
    "        df2 = df2.drop( cols_drop, axis=1 )\n",
    "        \n",
    "        return df2\n",
    "\n",
    "\n",
    "    def data_preparation( self, df5 ):\n",
    "\n",
    "        ## 5.2. Rescaling \n",
    "        # competition distance\n",
    "        df5['competition_distance'] = self.competition_distance_scaler.fit_transform( df5[['competition_distance']].values )\n",
    "    \n",
    "        # competition time month\n",
    "        df5['competition_time_month'] = self.competition_time_month_scaler.fit_transform( df5[['competition_time_month']].values )\n",
    "\n",
    "        # promo time week\n",
    "        df5['promo_time_week'] = self.promo_time_week_scaler.fit_transform( df5[['promo_time_week']].values )\n",
    "        \n",
    "        # year\n",
    "        df5['year'] = self.year_scaler.fit_transform( df5[['year']].values )\n",
    "\n",
    "        ### 5.3.1. Encoding\n",
    "        # state_holiday - One Hot Encoding\n",
    "        df5 = pd.get_dummies( df5, prefix=['state_holiday'], columns=['state_holiday'] )\n",
    "\n",
    "        # store_type - Label Encoding\n",
    "        df5['store_type'] = self.store_type_scaler.fit_transform( df5['store_type'] )\n",
    "\n",
    "        # assortment - Ordinal Encoding\n",
    "        assortment_dict = {'basic': 1,  'extra': 2, 'extended': 3}\n",
    "        df5['assortment'] = df5['assortment'].map( assortment_dict )\n",
    "\n",
    "        \n",
    "        ### 5.3.3. Nature Transformation\n",
    "        # day of week\n",
    "        df5['day_of_week_sin'] = df5['day_of_week'].apply( lambda x: np.sin( x * ( 2. * np.pi/7 ) ) )\n",
    "        df5['day_of_week_cos'] = df5['day_of_week'].apply( lambda x: np.cos( x * ( 2. * np.pi/7 ) ) )\n",
    "\n",
    "        # month\n",
    "        df5['month_sin'] = df5['month'].apply( lambda x: np.sin( x * ( 2. * np.pi/12 ) ) )\n",
    "        df5['month_cos'] = df5['month'].apply( lambda x: np.cos( x * ( 2. * np.pi/12 ) ) )\n",
    "\n",
    "        # day \n",
    "        df5['day_sin'] = df5['day'].apply( lambda x: np.sin( x * ( 2. * np.pi/30 ) ) )\n",
    "        df5['day_cos'] = df5['day'].apply( lambda x: np.cos( x * ( 2. * np.pi/30 ) ) )\n",
    "\n",
    "        # week of year\n",
    "        df5['week_of_year_sin'] = df5['week_of_year'].apply( lambda x: np.sin( x * ( 2. * np.pi/52 ) ) )\n",
    "        df5['week_of_year_cos'] = df5['week_of_year'].apply( lambda x: np.cos( x * ( 2. * np.pi/52 ) ) )\n",
    "        \n",
    "        \n",
    "        cols_selected = [ 'store', 'promo', 'store_type', 'assortment', 'competition_distance', 'competition_open_since_month',\n",
    "            'competition_open_since_year', 'promo2', 'promo2_since_week', 'promo2_since_year', 'competition_time_month', 'promo_time_week',\n",
    "            'day_of_week_sin', 'day_of_week_cos', 'month_sin', 'month_cos', 'day_sin', 'day_cos', 'week_of_year_sin', 'week_of_year_cos']\n",
    "        \n",
    "        return df5[ cols_selected ]\n",
    "    \n",
    "    \n",
    "    def get_prediction( self, model, original_data, test_data ):\n",
    "        # prediction\n",
    "        pred = model.predict( test_data )\n",
    "        \n",
    "        # join pred into the original data\n",
    "        original_data['prediction'] = np.expm1( pred )\n",
    "        \n",
    "        return original_data.to_json( orient='records', date_format='iso' ) # iso: não bagunçar a data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.2. API Handler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-23T02:14:35.879937Z",
     "start_time": "2023-04-23T02:14:35.879937Z"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "from flask             import Flask, request, Response\n",
    "from rossmann.Rossmann import Rossmann\n",
    "\n",
    "# loading model\n",
    "model = pickle.load( open( '/Users/meigarom/repos/DataScience_Em_Producao/model/model_rossmann.pkl', 'rb') )\n",
    "\n",
    "# initialize API\n",
    "app = Flask( __name__ )\n",
    "\n",
    "@app.route( '/rossmann/predict', methods=['POST'] )\n",
    "def rossmann_predict():\n",
    "    test_json = request.get_json()\n",
    "   \n",
    "    if test_json: # there is data\n",
    "        if isinstance( test_json, dict ): # unique example\n",
    "            test_raw = pd.DataFrame( test_json, index=[0] )\n",
    "            \n",
    "        else: # multiple example\n",
    "            test_raw = pd.DataFrame( test_json, columns=test_json[0].keys() )\n",
    "            \n",
    "        # Instantiate Rossmann class\n",
    "        pipeline = Rossmann()\n",
    "        \n",
    "        # data cleaning\n",
    "        df1 = pipeline.data_cleaning( test_raw )\n",
    "        \n",
    "        # feature engineering\n",
    "        df2 = pipeline.feature_engineering( df1 )\n",
    "        \n",
    "        # data preparation\n",
    "        df3 = pipeline.data_preparation( df2 )\n",
    "        \n",
    "        # prediction\n",
    "        df_response = pipeline.get_prediction( model, test_raw, df3 )\n",
    "        \n",
    "        return df_response\n",
    "                \n",
    "    else:\n",
    "        return Reponse('{}', status=200, mimetype='application/json' )\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run( '0.0.0.0' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.3. API Tester"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-23T02:14:35.882936Z",
     "start_time": "2023-04-23T02:14:35.882936Z"
    }
   },
   "outputs": [],
   "source": [
    "# loading test dataset\n",
    "df10 = pd.read_csv( '/Users/meigarom/repos/DataScience_Em_Producao/data/test.csv' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-23T02:14:35.883935Z",
     "start_time": "2023-04-23T02:14:35.883935Z"
    }
   },
   "outputs": [],
   "source": [
    "# merge test dataset + store (perfomance das lojas + características das lojas)\n",
    "df_test = pd.merge( df10, df_store_raw, how='left', on='Store' )\n",
    "\n",
    "# choose store for prediction\n",
    "df_test = df_test[df_test['Store'].isin( [20, 23, 22] )]\n",
    "\n",
    "# remove closed days\n",
    "df_test = df_test[df_test['Open'] != 0]\n",
    "df_test = df_test[~df_test['Open'].isnull()] # ~: diferença\n",
    "df_test = df_test.drop( 'Id', axis=1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-23T02:14:35.885934Z",
     "start_time": "2023-04-23T02:14:35.885934Z"
    }
   },
   "outputs": [],
   "source": [
    "# convert Dataframe to json\n",
    "\n",
    "# pois para comunicação de dados entre sistemas deve ser json ou xml.\n",
    "# o sistema 1 converte o arquivo dataframe em json para enviar via API para o sistema 2\n",
    "# então, o sistema 2 converte novamente em dataframe para poder trabalhar\n",
    "\n",
    "# Orientação 'records': linha 1 - {'nome_coluna_1': valor_linha_1, 'nome_coluna_2' = valor_linha_1...},\n",
    "#                       linha 2 - {'nome_coluna_1': valor_linha_2, 'nome_coluna_2' = valor_linha_2...},\n",
    "\n",
    "data = json.dumps( df_test.to_dict( orient='records' ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-23T02:14:35.887933Z",
     "start_time": "2023-04-23T02:14:35.887933Z"
    }
   },
   "outputs": [],
   "source": [
    "# API Call\n",
    "#url = 'http://0.0.0.0:5000/rossmann/predict' # 5000: porta padrão do flask\n",
    "url = 'https://rossmann-model-test.herokuapp.com/rossmann/predict' # até antes de publicas no heroku usar a linha anterior\n",
    "header = {'Content-type': 'application/json' } \n",
    "data = data\n",
    "\n",
    "r = requests.post( url, data=data, headers=header ) # post: precisa enviar algum dado para requerir\n",
    "print( 'Status Code {}'.format( r.status_code ) ) # para ver se a requisição é válida ou não \n",
    "# status code: erro 404 - código devolvido pela API quando um servidor está desligado\n",
    "# status code = 200 (definido na seção 10.2) - quando devolver este valor quer dizer que está tudo certo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-23T02:14:35.889931Z",
     "start_time": "2023-04-23T02:14:35.889931Z"
    }
   },
   "outputs": [],
   "source": [
    "d1 = pd.DataFrame( r.json(), columns=r.json()[0].keys() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-23T02:14:35.890931Z",
     "start_time": "2023-04-23T02:14:35.890931Z"
    }
   },
   "outputs": [],
   "source": [
    "# quanto irá vender no total após 6 semanas\n",
    "\n",
    "d2 = d1[['store', 'prediction']].groupby( 'store' ).sum().reset_index()\n",
    "\n",
    "for i in range( len( d2 ) ): # quantidade de lojas escolhidas para fazer as predições\n",
    "    print( 'Store Number {} will sell R${:,.2f} in the next 6 weeks'.format( \n",
    "            d2.loc[i, 'store'], \n",
    "            d2.loc[i, 'prediction'] ) )\n",
    "    \n",
    "# ao rodar, mostra a loja e o valor que será vendido\n",
    "# troca no snippet do merge o 'isin', ou seja troca o 20, 22 e 23 pelas lojas que você quer ver e roda os códigos abaixo novamente"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
